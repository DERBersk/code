{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0102a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM3 modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Add sam3 to path assuming it is in the parent directory or ../sam3\n",
    "sys.path.append('..') \n",
    "sys.path.append('../sam3')\n",
    "\n",
    "try:\n",
    "    from sam3.model_builder import build_sam3_image_model\n",
    "    from sam3.model.sam3_image_processor import Sam3Processor\n",
    "    print(\"SAM3 modules imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing SAM3: {e}. Make sure the sam3 folder is correctly located.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05440a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM3 Model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the SAM3 Model\n",
    "# This matches the logic from preprocessing/colab.ipynb\n",
    "print(\"Loading SAM3 Model...\")\n",
    "model = build_sam3_image_model(enable_inst_interactivity=True)\n",
    "processor = Sam3Processor(model)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cbb4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing function defined.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_scanpath(parquet_path, dist_thresh=0.05, duration_thresh_ms=100):\n",
    "    \"\"\"\n",
    "    Preprocesses scanpaths:\n",
    "    1. Combines consecutive points if they are spatially close (dist < dist_thresh).\n",
    "    2. Filters out points that have too low duration (duration < duration_thresh_ms).\n",
    "    \n",
    "    Args:\n",
    "        parquet_path: Path to parquet file.\n",
    "        dist_thresh: Normalized distance threshold (0-1) to merge consecutive points.\n",
    "        duration_thresh_ms: Minimum duration in ms to keep a fixation.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['x', 'y', 'start_time', 'end_time', 'duration']\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Ensure sorted by time\n",
    "    df = df.sort_values('timestamp_ms')\n",
    "    \n",
    "    # Extract arrays\n",
    "    xs = df['x'].values\n",
    "    ys = df['y'].values\n",
    "    ts = df['timestamp_ms'].values\n",
    "    \n",
    "    fixations = []\n",
    "    \n",
    "    if len(xs) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Initial current group\n",
    "    curr_x_sum = xs[0]\n",
    "    curr_y_sum = ys[0]\n",
    "    curr_count = 1\n",
    "    curr_start = ts[0]\n",
    "    # Estimate raw sample duration as avg diff or diff to next\n",
    "    # For the last point, we'll assume same as prev interval or small default\n",
    "    \n",
    "    # Iterate\n",
    "    for i in range(1, len(xs)):\n",
    "        # Calculate duration of PREVIOUS point (timestamp difference)\n",
    "        # Note: simplistic duration calc: T[i] - T[i-1] for the (i-1)th sample\n",
    "        dt = ts[i] - ts[i-1]\n",
    "        \n",
    "        # Calculate instantaneous centroid of current group\n",
    "        curr_x = curr_x_sum / curr_count\n",
    "        curr_y = curr_y_sum / curr_count\n",
    "        \n",
    "        dist = np.sqrt((xs[i] - curr_x)**2 + (ys[i] - curr_y)**2)\n",
    "        \n",
    "        if dist < dist_thresh:\n",
    "            # Combine\n",
    "            curr_x_sum += xs[i]\n",
    "            curr_y_sum += ys[i]\n",
    "            curr_count += 1\n",
    "        else:\n",
    "            # Finish group\n",
    "            # Group duration is Current Timestamp - Start Timestamp\n",
    "            # (Strictly speaking, it subsumes the intervals between Start and Current)\n",
    "            grp_duration = ts[i-1] - curr_start + (ts[i] - ts[i-1]) # Include last sample duration roughly\n",
    "            \n",
    "            centroid_x = curr_x_sum / curr_count\n",
    "            centroid_y = curr_y_sum / curr_count\n",
    "            \n",
    "            fixations.append({\n",
    "                'x': centroid_x,\n",
    "                'y': centroid_y,\n",
    "                'start_time': curr_start,\n",
    "                'end_time': ts[i], # Approx end\n",
    "                'duration': grp_duration\n",
    "            })\n",
    "            \n",
    "            # Start new group\n",
    "            curr_x_sum = xs[i]\n",
    "            curr_y_sum = ys[i]\n",
    "            curr_count = 1\n",
    "            curr_start = ts[i]\n",
    "\n",
    "    # Append last group\n",
    "    if curr_count > 0:\n",
    "        # Default duration for last point?\n",
    "        grp_duration = 33 # Approx 30ms for last frame/sample\n",
    "        centroid_x = curr_x_sum / curr_count\n",
    "        centroid_y = curr_y_sum / curr_count\n",
    "        fixations.append({\n",
    "            'x': centroid_x,\n",
    "            'y': centroid_y,\n",
    "            'start_time': curr_start,\n",
    "            'end_time': curr_start + grp_duration,\n",
    "            'duration': grp_duration\n",
    "        })\n",
    "        \n",
    "    fix_df = pd.DataFrame(fixations)\n",
    "    \n",
    "    # Filter by duration\n",
    "    if not fix_df.empty:\n",
    "        fix_df = fix_df[fix_df['duration'] >= duration_thresh_ms].reset_index(drop=True)\n",
    "        \n",
    "    return fix_df\n",
    "\n",
    "print(\"Preprocessing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b818136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_for_fixations(image_path, fixations, processor):\n",
    "    \"\"\"\n",
    "    Generates a mask for each fixation point using SAM3.\n",
    "    \"\"\"\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = image_pil.size\n",
    "    \n",
    "    # Process image once? SAM3 processor might handle caching, or we call it per prompt.\n",
    "    # Looking at sam3 examples, usually we can pass points to the prompt.\n",
    "    \n",
    "    # Prepare inputs\n",
    "    # processor expects image. \n",
    "    # For simplicity, we process row by row to allow \"per point\" segmentation selection\n",
    "    \n",
    "    all_masks = []\n",
    "    \n",
    "    print(f\"Generating masks for {len(fixations)} fixations...\")\n",
    "\n",
    "    inference_state = processor.set_image(image_pil)\n",
    "\n",
    "    for idx, row in fixations.iterrows():\n",
    "        x_px = int(row['x'] * width)\n",
    "        y_px = int(row['y'] * height)\n",
    "        \n",
    "        # Clip to image bounds\n",
    "        x_px = max(0, min(width-1, x_px))\n",
    "        y_px = max(0, min(height-1, y_px))\n",
    "        \n",
    "        # Construct prompt\n",
    "        # SAM3 API: expects points like [[x, y]] and labels like [1]\n",
    "        input_points = [[[x_px, y_px]]]\n",
    "        input_labels = [[1]]\n",
    "\n",
    "        print('Pre-Fit')\n",
    "                \n",
    "        masks, scores, logits = model.predict_inst(\n",
    "            inference_state,\n",
    "            point_coords=input_points,\n",
    "            point_labels=input_labels,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        sorted_ind = np.argsort(scores)[::-1]\n",
    "        masks = masks[sorted_ind]\n",
    "        scores = scores[sorted_ind]\n",
    "        logits = logits[sorted_ind]\n",
    "\n",
    "        print('Post-Fit')\n",
    "        \n",
    "        ## 1. Standardize mask shape (handling 4D, 3D, or 2D inputs)\n",
    "        if len(masks.shape) == 4:\n",
    "            masks = masks.squeeze(0)\n",
    "        if len(masks.shape) == 3 and masks.shape[0] == 1:\n",
    "            masks = masks.squeeze(0)\n",
    "        \n",
    "        # 2. Combine all masks into one single master mask\n",
    "        # We use np.max to ensure if masks overlap, they stay 'visible'\n",
    "        if len(masks.shape) == 3:\n",
    "            combined_mask = np.max(masks, axis=0)\n",
    "        else:\n",
    "            combined_mask = masks\n",
    "        \n",
    "        # Convert to numpy uint8 0 or 255\n",
    "        # Masks are typically logits, so > 0 check converts to boolean\n",
    "        mask_np = (combined_mask > 0).astype(np.uint8) * 255\n",
    "        \n",
    "        all_masks.append(mask_np)\n",
    "        \n",
    "        if idx % 5 == 0:\n",
    "            print(f\"Processed {idx}/{len(fixations)}\")\n",
    "            \n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de674212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def generate_scanpath_video(image_path, fixations, masks, output_video_path, fps=30):\n",
    "    # Use PIL for image manipulation as requested\n",
    "    base_pil = Image.open(image_path).convert(\"RGBA\")\n",
    "    width, height = base_pil.size\n",
    "    \n",
    "    # Create the background with the requested dark overlay (0,0,0,200)\n",
    "    overlay = Image.new(\"RGBA\", base_pil.size, (0, 0, 0, 200))\n",
    "    dimmed_bg_pil = Image.alpha_composite(base_pil, overlay)\n",
    "    \n",
    "    if fixations.empty:\n",
    "        print(\"No fixations to animate.\")\n",
    "        return\n",
    "\n",
    "    min_time = fixations['start_time'].min()\n",
    "    max_time = fixations['end_time'].max()\n",
    "    duration_ms = max_time - min_time\n",
    "    total_frames = int((duration_ms / 1000.0) * fps)\n",
    "    \n",
    "    # VideoWriter expects BGR, create it\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Generating video: {output_video_path}\")\n",
    "    print(f\"Duration: {duration_ms}ms, Frames: {total_frames}\")\n",
    "    \n",
    "    # Dot radius (dynamic based on size, ~1.5% of max dimension)\n",
    "    radius = int(max(width, height) * 0.01)\n",
    "    \n",
    "    for f in range(total_frames):\n",
    "        current_time_ms = min_time + (f / fps) * 1000.0\n",
    "        \n",
    "        # Find active fixation\n",
    "        active_fix = fixations[\n",
    "            (fixations['start_time'] <= current_time_ms) & \n",
    "            (fixations['end_time'] >= current_time_ms)\n",
    "        ]\n",
    "        \n",
    "        # Start matching active mask on dimmed\n",
    "        frame_pil = dimmed_bg_pil.copy()\n",
    "        \n",
    "        if not active_fix.empty:\n",
    "            idx = active_fix.index[0]\n",
    "            if idx < len(masks):\n",
    "                mask_np = masks[idx]\n",
    "                \n",
    "                # Resize if needed\n",
    "                if mask_np.shape[:2] != (height, width):\n",
    "                    mask_np = cv2.resize(mask_np, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Convert mask to PIL (L mode) for compositing\n",
    "                mask_pil = Image.fromarray(mask_np).convert(\"L\")\n",
    "                \n",
    "                # Composite: Show base_pil (bright) where mask is white, dimmed_bg elsewhere\n",
    "                frame_pil = Image.composite(base_pil, frame_pil, mask_pil)\n",
    "        \n",
    "        # Overlay drawing layer (for dots and lines with proper alpha blending)\n",
    "        shapes_layer = Image.new(\"RGBA\", (width, height), (0, 0, 0, 0))\n",
    "        draw = ImageDraw.Draw(shapes_layer)\n",
    "\n",
    "        # Collect points for the path (all fixations started up to now)\n",
    "        # Includes finished grey ones and potentially the active red one.\n",
    "        # \"Connect all grey and the red dot\"\n",
    "        \n",
    "        # 1. Previous fixations (Grey)\n",
    "        prev_fixs = fixations[fixations['end_time'] < current_time_ms]\n",
    "        for _, row in prev_fixs.iterrows():\n",
    "            px, py = row['x'] * width, row['y'] * height\n",
    "            draw.ellipse((px - radius, py - radius, px + radius, py + radius), fill=(128, 128, 128, 100))\n",
    "        \n",
    "        # 2. Current fixation (Red)\n",
    "        if not active_fix.empty:\n",
    "            row = active_fix.iloc[0]\n",
    "            px, py = row['x'] * width, row['y'] * height\n",
    "            draw.ellipse((px - radius, py - radius, px + radius, py + radius), fill=(255, 0, 0, 200))\n",
    "            \n",
    "        # Composite shapes onto frame\n",
    "        frame_pil = Image.alpha_composite(frame_pil, shapes_layer)\n",
    "        \n",
    "        # Convert PIL RGBA -> RGB -> BGR for OpenCV\n",
    "        frame_rgb = frame_pil.convert(\"RGB\")\n",
    "        frame_bgr = np.array(frame_rgb)[:, :, ::-1]\n",
    "        \n",
    "        out.write(frame_bgr)\n",
    "        \n",
    "        if f % 30 == 0:\n",
    "            print(f\"Frame {f}/{total_frames}\", end='\\r')\n",
    "\n",
    "    out.release()\n",
    "    print(\"\\nVideo saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67aba040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../parquet\\P01_IMG004_10100.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Example selection. Change these to select different user/image.\n",
    "PARQUET_FOLDER = r'../parquet'\n",
    "IMAGE_FOLDER = r'../img_bin'\n",
    "OUTPUT_FOLDER = r'./output_videos'\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "# ID Helper\n",
    "# Parquet format: P22_IMG036_00100.parquet\n",
    "# Image format: IMG036_00100.jpg\n",
    "# Let's pick P22 and IMG036 for demo\n",
    "TARGET_FILE_NAME = \"P01_IMG004_10100.parquet\"\n",
    "TARGET_IMG_NAME = \"IMG004_10100.jpg\"\n",
    "\n",
    "parquet_path = os.path.join(PARQUET_FOLDER, TARGET_FILE_NAME)\n",
    "image_path = os.path.join(IMAGE_FOLDER, TARGET_IMG_NAME)\n",
    "\n",
    "print(f\"Processing: {parquet_path}\")\n",
    "\n",
    "# 1. Preprocess\n",
    "# dist_thresh=0.05 (5% of screen diagonal approx?), duration_thresh=100ms\n",
    "fixations_df = preprocess_scanpath(parquet_path, dist_thresh=0.05, duration_thresh_ms=80) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034931ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 fixations after preprocessing.\n",
      "Generating masks for 15 fixations...\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Processed 0/15\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Processed 5/15\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Processed 10/15\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Pre-Fit\n",
      "Post-Fit\n",
      "Generating video: ./output_videos\\P01_IMG004_10100.mp4\n",
      "Duration: 5500.0ms, Frames: 165\n",
      "Frame 150/165\n",
      "Video saved.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(fixations_df)} fixations after preprocessing.\")\n",
    "# print(fixations_df.head())\n",
    "\n",
    "if not fixations_df.empty:\n",
    "    # 2. Extract Masks\n",
    "    masks = get_masks_for_fixations(image_path, fixations_df, processor)\n",
    "    \n",
    "    # 3. Generate Video\n",
    "    video_name = TARGET_FILE_NAME.replace('.parquet', '.mp4')\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, video_name)\n",
    "    \n",
    "    generate_scanpath_video(image_path, fixations_df, masks, output_path, fps=30)\n",
    "else:\n",
    "    print(\"Skipping video generation due to lack of valid fixations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6456308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading video...\n",
      "Processing...\n",
      "Video is ready!\n",
      "------------------------------\n",
      "Is that Trump's face? And there's Melania... Wait, back to him. Why is his head a building? Oh my god. That's one of the Twin Towers, and he's holding airplanes. That is... rough. And who's this other guy?\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import time\n",
    "\n",
    "# Initialize the client\n",
    "# (It automatically looks for the GEMINI_API_KEY environment variable)\n",
    "client = genai.Client(api_key=\"Your_Google_API_Key_Here\")\n",
    "\n",
    "# 1. Upload the video\n",
    "# This is required for videos larger than 20MB or longer than 1 minute\n",
    "print(\"Uploading video...\")\n",
    "video_file = client.files.upload(file=r\"C:\\Users\\domin\\OneDrive\\Seminar\\code\\video_generation\\output_videos\\P01_IMG002_10100.mp4\")\n",
    "\n",
    "# 2. Wait for processing\n",
    "# Video files must be in 'ACTIVE' state before they can be used in a prompt\n",
    "while video_file.state.name == \"PROCESSING\":\n",
    "    print(\"Processing...\", end=\"\\r\")\n",
    "    time.sleep(2)\n",
    "    video_file = client.files.get(name=video_file.name)\n",
    "\n",
    "print(\"\\nVideo is ready!\")\n",
    "\n",
    "# 3. Prompt with Text + Video\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\", \n",
    "    contents=[\n",
    "        video_file,\n",
    "        \"\"\"You are tasked with generating a short inner monologue (approx 15 seconds when read) based on the following structured inputs:\n",
    "-Video description: A social media image, humorous or satirical in tone, displayed with an animated visual path based on eye-tracking data.\n",
    "-Eye-tracking sequence: We have a series of visual items (extracted via Facebook’s SAM segmentation), each corresponding to where the viewer looked, in chronological order.\n",
    "\n",
    "Instructions for generating the inner monologue:\n",
    "-Imagine you’re the viewer whose gaze follows that exact sequence.\n",
    "-The monologue should reflect a quick, almost instantaneous train of thought.\n",
    "-Connect the items naturally, as if noticing one thing after another, with reactions.\n",
    "-Be concise — the whole monologue should last about 15 seconds if spoken aloud.\n",
    "-No need to describe the eye movement itself; just the inner thoughts triggered by what’s seen.\n",
    "\n",
    "Example input format you will receive:\n",
    "-Items in gaze order: [cat with sunglasses, text: “I hate Mondays”, empty coffee mug, explosion in background]\n",
    "-Tone: humor\n",
    "-Example output monologue:\n",
    "“A cat with shades, cool. ‘I hate Mondays’ — yeah, me too. Wait, why’s the mug empty? Oh. Explains the explosion.”\n",
    "\n",
    "Now generate for the video. Only respond with the monologue text, no extra commentary.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(response.text)\n",
    "\n",
    "# 4. Cleanup (Optional)\n",
    "# Files are auto-deleted after 48 hours, but you can delete manually\n",
    "resp = client.files.delete(name=video_file.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
