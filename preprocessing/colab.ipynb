{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da48fc6d",
   "metadata": {},
   "source": [
    "# Example Initial Pipeline for Saliency Object Detection based on Eye Tracking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('sam3'):\n",
    "    !git clone https://github.com/facebookresearch/sam3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd sam3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4967d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e \".[notebooks]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32f2a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ae8af2ce154d5a9b47005de526fce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fdd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#################################### For Image ####################################\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "# Load the model\n",
    "model = build_sam3_image_model(enable_inst_interactivity=True)\n",
    "processor = Sam3Processor(model)\n",
    "# Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c426d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fixation_centroids import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52c816",
   "metadata": {},
   "source": [
    "fixation -> centroids points for prompting model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant = 'P01'\n",
    "path = '/content/drive/MyDrive/Colab Notebooks/my_projects/sam/fixations'\n",
    "fixation_data = {}\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.parquet') and file.startswith(participant):\n",
    "        if not file.startswith(\"P01_IMG001\"):\n",
    "          continue\n",
    "        full_path = os.path.join(path, file)\n",
    "        centroids = calculate_fixation_centroids(full_path)\n",
    "        image_name = file.replace(f'{participant}_', '').replace('.parquet', '.jpg')\n",
    "\n",
    "        # Store data in dictionary\n",
    "        if image_name not in fixation_data:\n",
    "            fixation_data[image_name] = {\n",
    "                'participants': [],\n",
    "                'centroids': []\n",
    "            }\n",
    "\n",
    "        fixation_data[image_name]['participants'].append(participant)\n",
    "        fixation_data[image_name]['centroids'].append(centroids)\n",
    "fixation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39743d3a",
   "metadata": {},
   "source": [
    "overlay mask for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c939fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "def overlay_masks(image, masks):\n",
    "    image = image.convert(\"RGBA\")\n",
    "\n",
    "    # Handle different mask shapes - squeeze out batch and channel dimensions if present\n",
    "    if len(masks.shape) == 4:\n",
    "        masks = masks.squeeze(0)  # Remove batch dimension\n",
    "    if len(masks.shape) == 3 and masks.shape[0] == 1:\n",
    "        masks = masks.squeeze(0)  # Remove channel dimension if it's 1\n",
    "\n",
    "    # Convert to numpy and scale to 0-255\n",
    "    masks_np = 255 * masks.astype(np.uint8)\n",
    "\n",
    "    # If we have a single mask, add a dimension\n",
    "    if len(masks_np.shape) == 2:\n",
    "        masks_np = masks_np[np.newaxis, ...]\n",
    "\n",
    "    n_masks = masks_np.shape[0]\n",
    "    cmap = matplotlib.colormaps.get_cmap(\"rainbow\").resampled(n_masks)\n",
    "    colors = [\n",
    "        tuple(int(c * 255) for c in cmap(i)[:3])\n",
    "        for i in range(n_masks)\n",
    "    ]\n",
    "\n",
    "    for mask, color in zip(masks_np, colors):\n",
    "        mask = Image.fromarray(mask)\n",
    "        overlay = Image.new(\"RGBA\", image.size, color + (0,))\n",
    "        alpha = mask.point(lambda v: int(v * 0.5))\n",
    "        overlay.putalpha(alpha)\n",
    "        image = Image.alpha_composite(image, overlay)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2fa95",
   "metadata": {},
   "source": [
    "predict and store the original image with the mask overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks= None\n",
    "scores=None\n",
    "logits= None\n",
    "image = None\n",
    "\n",
    "for image_filename in fixation_data:\n",
    "  nr_images +=1\n",
    "  full_image_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/my_projects/sam/img_bin',image_filename)\n",
    "\n",
    "\n",
    "  image = Image.open(full_image_path)\n",
    "  width, height = image.size\n",
    "  inference_state = processor.set_image(image)\n",
    "  nr_centroids = 0\n",
    "  for centroid in fixation_data[image_filename].get(\"centroids\")[0]:# not multiple Participant yet so specify 0 works\n",
    "    print(centroid)\n",
    "    pixel_x = round(centroid[0] * width)\n",
    "    pixel_y = round(centroid[1] * height)\n",
    "    point = [[pixel_x, pixel_y]]\n",
    "\n",
    "    label = np.array([1])\n",
    "\n",
    "    masks, scores, logits = model.predict_inst(\n",
    "    inference_state,\n",
    "    point_coords=point,\n",
    "    point_labels=label,\n",
    "    multimask_output=False,\n",
    "    )\n",
    "\n",
    "    sorted_ind = np.argsort(scores)[::-1]\n",
    "    masks = masks[sorted_ind]\n",
    "    scores = scores[sorted_ind]\n",
    "    logits = logits[sorted_ind]\n",
    "\n",
    "    annotated_img = overlay_masks(image.copy(), masks=masks)\n",
    "\n",
    "   # Save the annotated image\n",
    "    path_example_img = '/content/drive/MyDrive/Colab Notebooks/my_projects/sam/example_outputs'\n",
    "    # Create filename using base image name + participant + centroid counter\n",
    "    base_name = os.path.splitext(image_filename)[0]  # Remove .jpg extension\n",
    "    output_filename = f\"{base_name}_participant0_centroid{nr_centroids}.png\"\n",
    "    output_path = os.path.join(path_example_img, output_filename)\n",
    "\n",
    "    annotated_img.save(output_path, 'PNG')\n",
    "    nr_centroids +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d8883",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
